{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano music generation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5JOD85ExRGmB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPUwUBim94oI",
    "outputId": "c696411a-0b5a-4b54-c77b-f2fdbbde086d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /Users/jean-baptistegourlet/opt/anaconda3/envs/ml37/lib/python3.7/site-packages (6.5.0)\n",
      "Collecting midiutil\n",
      "  Downloading MIDIUtil-1.2.1.tar.gz (1.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.0 MB 4.9 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: chardet in /Users/jean-baptistegourlet/opt/anaconda3/envs/ml37/lib/python3.7/site-packages (from music21) (4.0.0)\n",
      "Requirement already satisfied: more-itertools in /Users/jean-baptistegourlet/opt/anaconda3/envs/ml37/lib/python3.7/site-packages (from music21) (8.6.0)\n",
      "Requirement already satisfied: joblib in /Users/jean-baptistegourlet/opt/anaconda3/envs/ml37/lib/python3.7/site-packages (from music21) (1.0.0)\n",
      "Requirement already satisfied: webcolors in /Users/jean-baptistegourlet/opt/anaconda3/envs/ml37/lib/python3.7/site-packages (from music21) (1.11.1)\n",
      "Building wheels for collected packages: midiutil\n",
      "  Building wheel for midiutil (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for midiutil: filename=MIDIUtil-1.2.1-py3-none-any.whl size=54566 sha256=834cfc81bd2b3602b8cf57fd923ae8cb6ce6a6458b3c30c7666a6e89b17cb085\n",
      "  Stored in directory: /Users/jean-baptistegourlet/Library/Caches/pip/wheels/e3/97/cd/a677b61a76d575f373e2e10302f1d9106507fea6dd1320df03\n",
      "Successfully built midiutil\n",
      "Installing collected packages: midiutil\n",
      "Successfully installed midiutil-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install midiutil music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d9s4L7nM932j"
   },
   "outputs": [],
   "source": [
    "from midiutil import MIDIFile\n",
    "from music21 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hZVvcGNm3G2v"
   },
   "outputs": [],
   "source": [
    "def get_clips(directory, number_of_clips):\n",
    "    i=0\n",
    "    notes  = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if i<number_of_clips:\n",
    "            i+=1\n",
    "            print(filename)\n",
    "            note = pd.read_pickle(directory + str(filename))\n",
    "            notes += [note] \n",
    "        else:\n",
    "          break\n",
    "    return notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N-vI2FE-yTje"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_set(notes,vocab, n_vocab, sequence_length = 50):\n",
    "    pitchnames = vocab\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    if not isinstance(notes[0], list):\n",
    "       notes = [notes] \n",
    "    i=0\n",
    "    lag = sequence_length//7\n",
    "    for clip in notes:\n",
    "        for i in range(0,len(clip)-sequence_length):\n",
    "            sequence_in = clip[i:i + sequence_length]\n",
    "            sequence_out = clip[i + sequence_length]\n",
    "            network_input.append([note_to_int[char] for char in sequence_in])\n",
    "            network_output.append(note_to_int[sequence_out])\n",
    "    n_patterns = len(network_input)\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    return (network_input, network_output)\n",
    "\n",
    "def train(model, network_input, network_output, epochs=2, batch_size=200,validation_split=0.2):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    filepath = \"weights/best_model.h5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model.fit(network_input, network_output, epochs=epochs,\n",
    "              batch_size=batch_size, validation_split=validation_split, callbacks=callbacks_list)\n",
    "    \n",
    "    \n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4JqvhHKVyYIU"
   },
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab, depth = 3, dropout =0.3):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    if depth == 1:\n",
    "        model.add(LSTM(\n",
    "            256,\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "            recurrent_dropout=dropout\n",
    "        ))\n",
    "    if depth == 2:\n",
    "        model.add(LSTM(\n",
    "            256,\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "            recurrent_dropout=dropout,\n",
    "            return_sequences=True\n",
    "        ))\n",
    "        model.add(LSTM(512))\n",
    "        model.add(Dropout(dropout))\n",
    "    if depth == 3:\n",
    "        model.add(LSTM(\n",
    "            256,\n",
    "            input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "            recurrent_dropout=dropout,\n",
    "            return_sequences=True\n",
    "        ))\n",
    "        model.add(LSTM(512,return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WIsP46a982rZ"
   },
   "outputs": [],
   "source": [
    "def generate_notes(model, vocab, start_pattern):\n",
    "    print('generating notes')\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(vocab))\n",
    "    pattern = start_pattern\n",
    "    prediction_output = []\n",
    "    # generate 50 notes\n",
    "    for note_index in range(50):\n",
    "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        pattern = numpy.append(pattern,index/ float(n_vocab))\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        \n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D6nV2MoQGz4O"
   },
   "outputs": [],
   "source": [
    "def generate_random_notes(vocab, number):\n",
    "    notes = []\n",
    "    for n in range(number):\n",
    "        notes.append(random.choice(vocab))\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rBnAylrr-CfT"
   },
   "outputs": [],
   "source": [
    "def pitch_to_midi_number(sequence):\n",
    "    midi_notes = []\n",
    "    for p in sequence:\n",
    "        if ('.' in p):\n",
    "            notes_in_chord = p.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                notes.append(pitch.Pitch(current_note).midi)\n",
    "            midi_notes.append(notes)\n",
    "        else:\n",
    "            midi_notes.append(pitch.Pitch(p).midi)\n",
    "    return midi_notes\n",
    "\n",
    "def create_MIDI(prediction_output, output_name):\n",
    "    degrees  = pitch_to_midi_number(prediction_output)  # MIDI note number\n",
    "    track    = 0\n",
    "    channel  = 0\n",
    "    time     = 0    # In beats\n",
    "    duration = 1    # In beats\n",
    "    tempo    = 120  # In BPM\n",
    "    volume   = 100  # 0-127, as per the MIDI standard\n",
    "\n",
    "    MyMIDI = MIDIFile(1)  # One track, defaults to format 1 (tempo track is created\n",
    "                          \n",
    "    MyMIDI.addTempo(track, time, tempo)\n",
    "\n",
    "    for i, pitch in enumerate(degrees):\n",
    "        if type(pitch) is list:\n",
    "            for p in pitch:\n",
    "                MyMIDI.addNote(track, channel, p, time + i, duration, volume)\n",
    "        else:\n",
    "            MyMIDI.addNote(track, channel, pitch, time + i, duration, volume)\n",
    "\n",
    "    with open(output_name+'.mid', \"wb\") as output_file:\n",
    "        MyMIDI.writeFile(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMfyuVcNztBf"
   },
   "source": [
    "---\n",
    "\n",
    "# First shot: learn recurrent patterns and fine tune the model\n",
    "## 30 seconds of music\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHmMyniByGx7",
    "outputId": "e756b49e-aed6-42cc-aafb-87cb48c01b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes_2004_186\n",
      "notes_2006_111\n"
     ]
    }
   ],
   "source": [
    "directory = r'data/clips/'\n",
    "number_of_clips = 2\n",
    "\n",
    "notes = get_clips(directory, number_of_clips)\n",
    "\n",
    "all_dataset = sum(notes, [])\n",
    "\n",
    "train_set = notes[0:number_of_clips-1]\n",
    "\n",
    "small_pattern = train_set[0][0:240]*200\n",
    "vocab = sorted(set(small_pattern))\n",
    "n_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jean-baptistegourlet/Documents/code/DeepLearning/Music-Generation-Project'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTz9qsqU97c6"
   },
   "outputs": [],
   "source": [
    "sequence_length_to_test = [5,10,20,50,100,200,500]\n",
    "\n",
    "dict_history = {}\n",
    "for s in sequence_length_to_test:\n",
    "    network_input_train, network_output_train = create_set(small_pattern,vocab,n_vocab, sequence_length=s)\n",
    "    print('creating model for training, sequence_length = '+str(s))\n",
    "    model = create_network(network_input_train, n_vocab)\n",
    "    model_trained, history = train(model, network_input_train, network_output_train, validation_split=0.2, epochs=100, batch_size=128)\n",
    "    dict_history[s] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-3sl2lPGDIb"
   },
   "outputs": [],
   "source": [
    "depth_to_test = [1,2,3]\n",
    "\n",
    "dict_history = {}\n",
    "for d in depth_to_test:\n",
    "    network_input_train, network_output_train = create_set(small_pattern,vocab,n_vocab)\n",
    "    print('creating model for training, depth_to_test = '+str(d))\n",
    "    model = create_network(network_input_train, n_vocab, depth=d)\n",
    "    model_trained, history = train(model, network_input_train, network_output_train, validation_split=0.2, epochs=30, batch_size=128)\n",
    "    dict_history[d] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnw5zUVwGOqk"
   },
   "outputs": [],
   "source": [
    "dropout_to_test = [0,0.1,0.2,0.3,0.5,0.8]\n",
    "\n",
    "dict_history = {}\n",
    "for d in dropout_to_test:\n",
    "    network_input_train, network_output_train = create_set(small_pattern,vocab,n_vocab)\n",
    "    print('creating model for training, dropout_to_test = '+str(d))\n",
    "    model = create_network(network_input_train, n_vocab, dropout=d)\n",
    "    model_trained, history = train(model, network_input_train, network_output_train, validation_split=0.2, epochs=30, batch_size=128)\n",
    "    dict_history[d] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0YKjBwV6Q69"
   },
   "outputs": [],
   "source": [
    "batch_size_to_test = [32,128,252,512]\n",
    "\n",
    "dict_history = {}\n",
    "for b in batch_size_to_test:\n",
    "    network_input_train, network_output_train = create_set(small_pattern,vocab,n_vocab)\n",
    "    print('creating model for training, batch size = '+str(b))\n",
    "    model = create_network(network_input_train, n_vocab)\n",
    "    model_trained, history = train(model, network_input_train, network_output_train, validation_split=0.2, epochs=30, batch_size=b)\n",
    "    dict_history[b] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "x2mXfB28IG6e"
   },
   "outputs": [],
   "source": [
    "test_set = generate_random_notes(vocab,300)\n",
    "network_input_test = create_set(test_set,vocab,n_vocab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oV_p0M16pfA",
    "outputId": "bd1ad1d5-ff7f-41b8-e6cf-99b9a4c8bc9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 50, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 120)               0         \n",
      "=================================================================\n",
      "Total params: 2,657,400\n",
      "Trainable params: 2,657,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "300/300 [==============================] - 50s 153ms/step - loss: 4.3774 - val_loss: 2.9395\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.93949, saving model to weights/best_model.h5\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 45s 151ms/step - loss: 2.4695 - val_loss: 0.7260\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.93949 to 0.72598, saving model to weights/best_model.h5\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.7683 - val_loss: 0.1508\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.72598 to 0.15077, saving model to weights/best_model.h5\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 45s 150ms/step - loss: 0.2750 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.15077 to 0.04168, saving model to weights/best_model.h5\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 46s 153ms/step - loss: 0.1579 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04168 to 0.01073, saving model to weights/best_model.h5\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 46s 152ms/step - loss: 0.0981 - val_loss: 1.8457\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01073\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 45s 151ms/step - loss: 0.1025 - val_loss: 0.3220\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01073\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 45s 149ms/step - loss: 0.0952 - val_loss: 6.0230e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.01073 to 0.00060, saving model to weights/best_model.h5\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 45s 151ms/step - loss: 0.0341 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00060\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 45s 150ms/step - loss: 0.0337 - val_loss: 6.0232e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00060\n"
     ]
    }
   ],
   "source": [
    "network_input_train, network_output_train = create_set(small_pattern,vocab,n_vocab)\n",
    "model = create_network(network_input_train, n_vocab)\n",
    "\n",
    "model_trained, history = train(model, network_input_train, network_output_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RjxWTljSlGx",
    "outputId": "ca9e4d90-bd53-46d2-bb30-8be0ee86f1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "generating notes\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"weights/best_model.h5\")\n",
    "generated_notes = generate_notes(model, vocab, start_pattern=network_input_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zIQH7DyT5eHT"
   },
   "outputs": [],
   "source": [
    "create_MIDI(test_set[0:50], 'input_for_simple_generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "j21DwFoVsODj"
   },
   "outputs": [],
   "source": [
    "create_MIDI(generated_notes, 'generated_music_with_simple_generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgZ_C0YASmgL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Feed the network with more data, generate complex music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HT-IUuHzSkZV",
    "outputId": "70590585-8943-44fe-c4e4-70748dd6fc9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notes_2002_0\n",
      "notes_2002_1\n"
     ]
    }
   ],
   "source": [
    "directory = r'data/results2'\n",
    "number_of_clips = 2\n",
    "\n",
    "notes = get_clips(directory, number_of_clips)\n",
    "\n",
    "all_dataset = sum(notes, [])\n",
    "\n",
    "train_set = []\n",
    "for n in notes:\n",
    "    train_set += n*100\n",
    "\n",
    "vocab = sorted(set(all_dataset))\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "test_set = generate_random_notes(vocab,300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hK0ukrg-Lkt-",
    "outputId": "e33098ac-08ae-49f7-eff9-ff40ff83694f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50, 256)           264192    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1714)              440498    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1714)              0         \n",
      "=================================================================\n",
      "Total params: 3,067,058\n",
      "Trainable params: 3,067,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4990/4990 [==============================] - 819s 162ms/step - loss: 5.7494 - val_loss: 5.2211\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.22106, saving model to weights/best_model.h5\n",
      "Epoch 2/10\n",
      "4990/4990 [==============================] - 741s 148ms/step - loss: 5.2826 - val_loss: 3.1622\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.22106 to 3.16224, saving model to weights/best_model.h5\n",
      "Epoch 3/10\n",
      "4990/4990 [==============================] - 738s 148ms/step - loss: 2.5354 - val_loss: 1.4766\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.16224 to 1.47664, saving model to weights/best_model.h5\n",
      "Epoch 4/10\n",
      "4990/4990 [==============================] - 734s 147ms/step - loss: 0.5692 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.47664 to 0.07672, saving model to weights/best_model.h5\n",
      "Epoch 5/10\n",
      "4990/4990 [==============================] - 736s 148ms/step - loss: 0.2512 - val_loss: 0.0229\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07672 to 0.02288, saving model to weights/best_model.h5\n",
      "Epoch 6/10\n",
      "4990/4990 [==============================] - 737s 148ms/step - loss: 0.1874 - val_loss: 0.1086\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02288\n",
      "Epoch 7/10\n",
      "4990/4990 [==============================] - 733s 147ms/step - loss: 0.1919 - val_loss: 0.0484\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02288\n",
      "Epoch 8/10\n",
      "4990/4990 [==============================] - 733s 147ms/step - loss: 0.2485 - val_loss: 0.0737\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02288\n",
      "Epoch 9/10\n",
      " 830/4990 [===>..........................] - ETA: 9:54 - loss: 0.2861"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-f8319c3b64eb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnetwork_input_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_vocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mmodel_trained\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhistory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnetwork_input_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnetwork_output_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m128\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-16-7383a5178906>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, network_input, network_output, epochs, batch_size, validation_split)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m     history = model.fit(network_input, network_output, epochs=epochs,\n\u001B[0;32m---> 38\u001B[0;31m               batch_size=batch_size, validation_split=validation_split, callbacks=callbacks_list)\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1098\u001B[0m                 _r=1):\n\u001B[1;32m   1099\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1100\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1101\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1102\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 828\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"xla\"\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    853\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    854\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 855\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    856\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    857\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2941\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   2942\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 2943\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   2944\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2945\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1917\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1918\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1919\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1920\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1921\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    558\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 560\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    561\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "network_input_train, network_output_train = create_set(train_set,vocab,n_vocab)\n",
    "model = create_network(network_input_train, n_vocab)\n",
    "\n",
    "model_trained, history = train(model, network_input_train, network_output_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EF9-taEFTGLb",
    "outputId": "1140d048-c8a0-47b2-894b-494da5e4623d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "generating notes\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"weights/best_model.h5\")\n",
    "\n",
    "network_input_test = create_set(test_set,vocab,n_vocab, sequence_length=50)[0]\n",
    "generated_notes = generate_notes(model, vocab, network_input_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1n3IV2Kxmk9I"
   },
   "outputs": [],
   "source": [
    "create_MIDI(test_set[0:50], 'input_for_complex_generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "l7nxd_4Qmn3l"
   },
   "outputs": [],
   "source": [
    "create_MIDI(generated_notes, 'generated_music_with_complex_generation')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "music-generation-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}